# Databricks notebook source
#from pyspark.sql import SparkSession
#from dbpp_cluster_lib import *
 
json_file_path="abfss://iex-stg@datalakeeastus2dev.dfs.core.windows.net/dtv_r_agtscheddtlact_intraday_src_json/data_dt=202203211111/Http.245903.20220321.111122305.dtv_r_agtscheddtlact_intraday"
df3=flattenJson(json_file_path=json_file_path)
display(df3)

# COMMAND ----------

df3=flattenJson(json_string=sample_json)

# COMMAND ----------

import json
from typing import Dict
from pyspark.sql.session import SparkSession
from pyspark.sql import DataFrame as SDF
from pyspark.sql.functions import *
from pyspark.sql.types import *


def flattenJson(json_file_path=None, json_string=None, json_schema=None):
    try:
        def rename_dataframe_cols(df: SDF, col_names: Dict[str, str]) -> SDF:
            # Rename all columns in dataframe
            return df.select(*[col(col_name).alias(col_names.get(col_name, col_name)) for col_name in df.columns])

        def update_column_names(df: SDF, index: int) -> SDF:
            df_temp = df
            all_cols = df_temp.columns
            new_cols = dict((column, f"{column}*{index}") for column in all_cols)
            df_temp = df_temp.transform(lambda df_x: rename_dataframe_cols(df_x, new_cols))
            return df_temp

        def flatten_json(df_arg: SDF, index: int = 1) -> SDF:
            # Flatten json in a spark dataframe using recursion

            # update all column names with index 1
            df = update_column_names(df_arg, index) if index == 1 else df_arg

            # get all field names from dataframe
            fields = df.schema.fields

            # get all columns in a dataframe
            for field in fields:
                data_type = str(field.dataType)
                column_name = field.name
                first_10_chars = data_type[0:10]

                # If it is an array column
                if first_10_chars == 'ArrayType(':
                    # explode arry column
                    df_temp = df.withColumn(column_name, explode_outer(col(column_name)))
                    return flatten_json(df_temp, index + 1)

                # if it is a json object
                elif first_10_chars == 'StructType':
                    current_col = column_name
                    append_str = current_col
                    # get data of current column
                    data_type_str = str(df.schema[current_col].dataType)

                    # change column name if current column name exists in data type string
                    df_temp = df.withColumnRenamed(column_name, column_name + "#1") \
                        if column_name in data_type_str else df
                    current_col = current_col + '#1' if column_name in data_type_str else current_col

                    # expand struct columns values
                    df_before_expanding = df_temp.select(f"{current_col}.*")
                    newly_gen_cols = df_before_expanding.columns

                    # find next level value of column
                    begin_index = append_str.rfind('*')
                    end_index = len(append_str)
                    level = append_str[begin_index + 1:end_index]
                    next_level = int(level) + 1

                    # Update column names with new level
                    custom_cols = dict((field, f"{append_str}->{field}*{next_level}") for field in newly_gen_cols)
                    df_temp2 = df_temp.select("*", f"{current_col}.*").drop(current_col)
                    df_temp3 = df_temp2.transform(lambda df_x: rename_dataframe_cols(df_x, custom_cols))
                    return flatten_json(df_temp3, index + 1)

            return df

        debugMsg("INFO", "START: flattenJson function")
        debugMsg("INFO", "json_file_path : <{}>".format(str(json_file_path)))
        debugMsg("INFO", "json_string : <{}>".format(str(json_string)))
        debugMsg("INFO", "json_schema : <{}>".format(str(json_schema)))
        if json_file_path is not None and json_string is None:
            debugMsg("INFO", "json_file_path is passed")
            json_var = json_file_path
        elif json_file_path is None and json_string is not None:
            debugMsg("INFO", "json_string is passed")
            sc = spark.sparkContext
            json_var = sc.parallelize([json_string])
        else:
            debugMsg("ERROR", "json_file_path or json_string should passed")
            return -1


        df = spark.read.option("multiline","true").json(json_var, schema=json_schema)
        df2 = flatten_json(df)
        debugMsg("INFO", "END: flattenJson function")
        return df2
    except Exception as e:
        debugMsg("ERROR", "Failed to flatten json due to below error:")
        debugMsg("ERROR", str(e))
        return -1

